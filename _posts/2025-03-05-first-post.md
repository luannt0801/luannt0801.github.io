---
title: "My First Blog Post"
date: 2024-03-08
categories: [AI, Machine Learning]
permalink: /category/AI/
author: "Luan Nguyen"
image: "/assets/imgs/default.jpg"
---

## Hugging Face Course

1. Transformer Models
    
    - **Natural Language Processing**
        Má»¥c tiÃªu cá»§a NLP khÃ´ng pháº£i lÃ  hiá»ƒu Ä‘Æ°á»£c Ã½ nghÄ©a cá»§a 1 tá»« riÃªng láº» mÃ  hiá»ƒu Ä‘Æ°á»£c context (ngá»¯ cáº£nh) cá»§a nhá»¯ng tá»« Ä‘Ã³.

        CÃ¡c task cá»§a NLP phá»• biáº¿n lÃ :

        - *Classifying whole sentences*: PhÃ¢n loáº¡i cáº£ cÃ¢u xem cÃ¡c cÃ¢u nÃ o lÃ  mail rÃ¡c, check ngá»¯ phÃ¡p, logic hai cÃ¢u.
        - *Classifying each world in a sentence*: XÃ¡c Ä‘á»‹nh cÃ¡c thÃ nh pháº§n ngá»¯ phÃ¡p cá»§a cÃ¢u (Danh tá»«, Ä‘á»™ng tá»« , tÃ­nh tá»«) hoáº·c cÃ¡c thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn (ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm, tá»• chá»©c).
        - *Generating text content*: HoÃ n thÃ nh má»™t promt vá»›i viá»‡c tá»± Ä‘á»™ng generate tá»«, Ä‘iá»n vÃ o chá»— trá»‘ng trong vÄƒn báº£n báº±ng cÃ¡c tá»« Ä‘Æ°á»£c che giáº¥u.
        - *Extracting an answer from a text*: TrÃ­ch xuáº¥t cÃ¢u tráº£ lá»i cho cÃ¢u há»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p trong ngá»¯ cáº£nh.
        - *Generating a new sentence from an input text*: Dá»‹ch vÄƒn báº£n sang ngÃ´n ngá»¯ khÃ¡c, tÃ³m táº¯t vÄƒn báº£n.

        Tuy nhiÃªn NLP khÃ´ng giá»›i háº¡n á»Ÿ vÄƒn báº£n viáº¿t, nÃ³ cÅ©ng giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c phá»©c táº¡p trong nháº­n dáº¡ng giá»ng nÃ³i vÃ  thá»‹ giÃ¡c mÃ¡y tÃ­nh (táº¡o báº£n ghi *máº«u Ã¢m thanh* hoáº·c *mÃ´ táº£ hÃ¬nh áº£nh*)

    - **Transformer, what can they do?**
        Transformer models Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ giáº£i quyáº¿t táº¥t cáº£ cÃ¡c task cá»§a NLP
        ![alt text](/image.png)

        [ThÆ° viá»‡n transformer](https://github.com/huggingface/transformers)

    - **How do Transformer work**

        Lá»‹ch sá»­ phÃ¡t triá»ƒn cá»§a mÃ´ hÃ¬nh transformer
        ![alt text](/image-1.png)

        Kiáº¿n trÃºc Transformer Ä‘Æ°á»£c giá»›i thiá»‡u vÃ o thÃ¡ng 6 nÄƒm 2017. Trá»ng tÃ¢m cá»§a nghiÃªn cá»©u ban Ä‘áº§u lÃ  cÃ¡c nhiá»‡m vá»¥ dá»‹ch thuáº­t. Tiáº¿p theo Ä‘Ã³ lÃ  sá»± ra máº¯t cá»§a má»™t sá»‘ mÃ´ hÃ¬nh cÃ³ áº£nh hÆ°á»Ÿng, bao gá»“m:
        - ThÃ¡ng 6 nÄƒm 2018: GPT, mÃ´ hÃ¬nh Transformer Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c Ä‘áº§u tiÃªn, Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tinh chá»‰nh cÃ¡c nhiá»‡m vá»¥ NLP khÃ¡c nhau vÃ  thu Ä‘Æ°á»£c káº¿t quáº£ tiÃªn tiáº¿n
        - ThÃ¡ng 10 nÄƒm 2018: BERT, má»™t mÃ´ hÃ¬nh lá»›n khÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c, mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ táº¡o ra cÃ¡c báº£n tÃ³m táº¯t cÃ¢u tá»‘t hÆ¡n (nhiá»u hÆ¡n vá» Ä‘iá»u nÃ y trong chÆ°Æ¡ng tiáº¿p theo!)
        - ThÃ¡ng 2 nÄƒm 2019: GPT-2, phiÃªn báº£n cáº£i tiáº¿n (vÃ  lá»›n hÆ¡n) cá»§a GPT chÆ°a Ä‘Æ°á»£c phÃ¡t hÃ nh cÃ´ng khai ngay láº­p tá»©c do trÆ°á»›c nhá»¯ng lo ngáº¡i vá» Ä‘áº¡o Ä‘á»©c
        - ThÃ¡ng 10 nÄƒm 2019: DistilBERT, má»™t phiÃªn báº£n cháº¯t lá»c cá»§a BERT nhanh hÆ¡n 60%, bá»™ nhá»› nháº¹ hÆ¡n 40% vÃ  váº«n giá»¯ Ä‘Æ°á»£c 97% hiá»‡u suáº¥t cá»§a BERT
        - ThÃ¡ng 10 nÄƒm 2019: BART vÃ  T5, hai mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c lá»›n sá»­ dá»¥ng cÃ¹ng kiáº¿n â€‹â€‹trÃºc nhÆ° máº«u Transformer nguyÃªn báº£n (máº«u Ä‘áº§u tiÃªn lÃ m nhÆ° váº­y)
        - ThÃ¡ng 5 nÄƒm 2020, GPT-3, má»™t phiÃªn báº£n tháº­m chÃ­ cÃ²n lá»›n hÆ¡n cá»§a GPT-2 cÃ³ kháº£ nÄƒng thá»±c hiá»‡n tá»‘t nhiá»u tÃ¡c vá»¥ khÃ¡c nhau mÃ  khÃ´ng cáº§n tinh chá»‰nh (gá»i lÃ  zero-shot learning)

        Transformer chia lÃ m 3 loáº¡i:
        - GPT -like (also called auto-regressive Transformer models)
        - BERT -like (also called auto-encoding Transformer models)
        - BART/T5 -like (also called sequence-to-sequence Transformer models)

        Táº¥t cáº£ cÃ¡c model transformer Ä‘á»u Ä‘Æ°á»£c **Ä‘Ã o táº¡o mÃ´ hÃ¬nh ngÃ´n ngá»¯ cÃ³ giÃ¡m sÃ¡t, tá»©c Ä‘Ã o táº¡o vá»›i 1 lÆ°á»£ng lá»›n vÄƒn báº£n thÃ´ theo kiá»ƒu tá»± giÃ¡m sÃ¡t.** Há»c tá»± giÃ¡m sÃ¡t cÃ³ nghÄ©a lÃ  má»¥c tiÃªu Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»± Ä‘á»™ng tá»« Ä‘áº§u vÃ o cá»§a mÃ´ hÃ¬nh **==> KhÃ´ng cáº§n thiáº¿t pháº£i gáº¯n nhÃ£n cho dá»¯ liá»‡u!**

        Trong quÃ¡ trÃ¬nh há»c chuyá»ƒn giao, mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh má»™t cÃ¡ch cÃ³ giÃ¡m sÃ¡t - sá»­ dá»¥ng nhÃ£n do con ngÆ°á»i chÃº thÃ­ch cho 1 má»¥c Ä‘Ã­ch nháº¥t Ä‘á»‹nh.
        ***=> MÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ nhÃ¢n quáº£ vÃ¬ Ä‘áº§u ra phá»¥ thuá»™c vÃ o Ä‘áº§u vÃ o trong quÃ¡ khá»© vÃ  hiá»‡n táº¡i chá»© khÃ´ng phá»¥ thuá»™c vÃ o Ä‘áº§u vÃ o trong tÆ°Æ¡ng lai.***

    - **Encoder Model**
        CÃ¡c mÃ´ hÃ¬nh mÃ£ hÃ³a chá»‰ sá»­ dá»¥ng bá»™ mÃ£ hÃ³a cá»§a mÃ´ hÃ¬nh Transformer. á» má»—i giai Ä‘oáº¡n, cÃ¡c lá»›p chÃº Ã½ cÃ³ thá»ƒ truy cáº­p táº¥t cáº£ cÃ¡c tá»« trong cÃ¢u Ä‘áº§u tiÃªn. CÃ¡c mÃ´ hÃ¬nh nÃ y thÆ°á»ng Ä‘Æ°á»£c mÃ´ táº£ lÃ  cÃ³ sá»± chÃº Ã½ â€œhai chiá»uâ€ vÃ  thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  cÃ¡c mÃ´ hÃ¬nh mÃ£ hÃ³a tá»± Ä‘á»™ng .

        QuÃ¡ trÃ¬nh Ä‘Ã o táº¡o trÆ°á»›c cÃ¡c mÃ´ hÃ¬nh nÃ y thÆ°á»ng xoay quanh viá»‡c lÃ m há»ng má»™t cÃ¢u nháº¥t Ä‘á»‹nh (vÃ­ dá»¥, báº±ng cÃ¡ch che giáº¥u cÃ¡c tá»« ngáº«u nhiÃªn trong cÃ¢u Ä‘Ã³) vÃ  giao cho mÃ´ hÃ¬nh nhiá»‡m vá»¥ tÃ¬m hoáº·c tÃ¡i táº¡o cÃ¢u ban Ä‘áº§u.

        CÃ¡c mÃ´ hÃ¬nh mÃ£ hÃ³a phÃ¹ há»£p nháº¥t cho cÃ¡c nhiá»‡m vá»¥ Ä‘Ã²i há»i pháº£i hiá»ƒu toÃ n bá»™ cÃ¢u, cháº³ng háº¡n nhÆ° phÃ¢n loáº¡i cÃ¢u, nháº­n dáº¡ng thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn (vÃ  nÃ³i chung lÃ  phÃ¢n loáº¡i tá»«) vÃ  tráº£ lá»i cÃ¢u há»i trÃ­ch xuáº¥t.

        Äáº¡i diá»‡n cá»§a nhÃ³m mÃ´ hÃ¬nh nÃ y bao gá»“m:

        [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)
        [BERT](https://huggingface.co/docs/transformers/model_doc/bert)
        [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)
        [ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)
        [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)

    - **Decoder model**
        CÃ¡c mÃ´ hÃ¬nh bá»™ giáº£i mÃ£ chá»‰ sá»­ dá»¥ng bá»™ giáº£i mÃ£ cá»§a mÃ´ hÃ¬nh Transformer. á» má»—i giai Ä‘oáº¡n, Ä‘á»‘i vá»›i má»™t tá»« nháº¥t Ä‘á»‹nh, cÃ¡c lá»›p chÃº Ã½ chá»‰ cÃ³ thá»ƒ truy cáº­p cÃ¡c tá»« Ä‘Æ°á»£c Ä‘áº·t trÆ°á»›c tá»« Ä‘Ã³ trong cÃ¢u. Nhá»¯ng mÃ´ hÃ¬nh nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  mÃ´ hÃ¬nh tá»± há»“i quy. Viá»‡c huáº¥n luyá»‡n trÆ°á»›c cÃ¡c mÃ´ hÃ¬nh giáº£i mÃ£ thÆ°á»ng xoay quanh viá»‡c dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong cÃ¢u. Nhá»¯ng mÃ´ hÃ¬nh nÃ y phÃ¹ há»£p nháº¥t cho cÃ¡c nhiá»‡m vá»¥ liÃªn quan Ä‘áº¿n viá»‡c táº¡o vÄƒn báº£n. Äáº¡i diá»‡n cá»§a gia Ä‘Ã¬nh ngÆ°á»i máº«u nÃ y bao gá»“m:

        [CTRL](https://huggingface.co/transformers/model_doc/ctrl)
        [GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)
        [GPT-2](https://huggingface.co/transformers/model_doc/gpt2)
        [Transformer XL](https://huggingface.co/transformers/model_doc/transfo-xl)

    
    - **Sequence to sequence model**
        CÃ¡c mÃ´ hÃ¬nh bá»™ mÃ£ hÃ³a-giáº£i mÃ£ (cÃ²n gá»i lÃ  mÃ´ hÃ¬nh tuáº§n tá»±) sá»­ dá»¥ng cáº£ hai pháº§n cá»§a kiáº¿n â€‹â€‹trÃºc Transformer. á» má»—i giai Ä‘oáº¡n, cÃ¡c lá»›p chÃº Ã½ cá»§a bá»™ mÃ£ hÃ³a cÃ³ thá»ƒ truy cáº­p táº¥t cáº£ cÃ¡c tá»« trong cÃ¢u Ä‘áº§u tiÃªn, trong khi cÃ¡c lá»›p chÃº Ã½ cá»§a bá»™ giáº£i mÃ£ chá»‰ cÃ³ thá»ƒ truy cáº­p cÃ¡c tá»« Ä‘Æ°á»£c Ä‘áº·t trÆ°á»›c má»™t tá»« nháº¥t Ä‘á»‹nh trong Ä‘áº§u vÃ o. Viá»‡c huáº¥n luyá»‡n trÆ°á»›c cÃ¡c mÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh bá»™ mÃ£ hÃ³a hoáº·c bá»™ giáº£i mÃ£, nhÆ°ng thÆ°á»ng liÃªn quan Ä‘áº¿n má»™t cÃ¡i gÃ¬ Ä‘Ã³ phá»©c táº¡p hÆ¡n má»™t chÃºt.
        
        VÃ­ dá»¥: T5 Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c báº±ng cÃ¡ch thay tháº¿ cÃ¡c Ä‘oáº¡n vÄƒn báº£n ngáº«u nhiÃªn (cÃ³ thá»ƒ chá»©a má»™t sá»‘ tá»«) báº±ng má»™t tá»« Ä‘áº·c biá»‡t máº·t náº¡ duy nháº¥t vÃ  má»¥c tiÃªu lÃ  dá»± Ä‘oÃ¡n vÄƒn báº£n mÃ  tá»« máº·t náº¡ nÃ y thay tháº¿.
        
        CÃ¡c mÃ´ hÃ¬nh tuáº§n tá»± phÃ¹ há»£p nháº¥t cho cÃ¡c nhiá»‡m vá»¥ xoay quanh viá»‡c táº¡o cÃ¢u má»›i tÃ¹y thuá»™c vÃ o Ä‘áº§u vÃ o nháº¥t Ä‘á»‹nh, cháº³ng háº¡n nhÆ° tÃ³m táº¯t, dá»‹ch thuáº­t hoáº·c tráº£ lá»i cÃ¢u há»i tá»•ng há»£p. Äáº¡i diá»‡n cá»§a gia Ä‘Ã¬nh ngÆ°á»i máº«u nÃ y bao gá»“m:

        [BART](https://huggingface.co/transformers/model_doc/bart)
        [mBART](https://huggingface.co/transformers/model_doc/mbart)
        [Marian](https://huggingface.co/transformers/model_doc/marian)
        [T5](https://huggingface.co/transformers/model_doc/t5)


    - **Bias and Limitation**

        Náº¿u báº¡n cÃ³ Ã½ Ä‘á»‹nh sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c hoáº·c phiÃªn báº£n tinh chá»‰nh trong sáº£n xuáº¥t, hÃ£y lÆ°u Ã½ ráº±ng, máº·c dÃ¹ cÃ¡c mÃ´ hÃ¬nh nÃ y lÃ  nhá»¯ng cÃ´ng cá»¥ máº¡nh máº½, nhÆ°ng chÃºng cÅ©ng cÃ³ nhá»¯ng háº¡n cháº¿. Háº¡n cháº¿ lá»›n nháº¥t lÃ , Ä‘á»ƒ cÃ³ thá»ƒ Ä‘Ã o táº¡o trÆ°á»›c trÃªn lÆ°á»£ng dá»¯ liá»‡u lá»›n, cÃ¡c nhÃ  nghiÃªn cá»©u thÆ°á»ng thu tháº­p táº¥t cáº£ ná»™i dung há» cÃ³ thá»ƒ tÃ¬m tháº¥y, láº¥y cáº£ ná»™i dung tá»‘t nháº¥t vÃ  tá»‡ nháº¥t cÃ³ sáºµn trÃªn internet.

        Äá»ƒ minh há»a nhanh, chÃºng ta hÃ£y quay láº¡i vÃ­ dá»¥ vá» fill-maskÄ‘Æ°á»ng á»‘ng vá»›i mÃ´ hÃ¬nh BERT:
        ```python
        from transformers import pipeline

        unmasker = pipeline("fill-mask", model="bert-base-uncased")
        result = unmasker("This man works as a [MASK].")
        print([r["token_str"] for r in result])

        result = unmasker("This woman works as a [MASK].")
        print([r["token_str"] for r in result])
        ```

        output
        ```python
        ['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
        ['nurse', 'waitress', 'teacher', 'maid', 'prostitute']
        ```

        Khi Ä‘Æ°á»£c yÃªu cáº§u Ä‘iá»n tá»« cÃ²n thiáº¿u trong hai cÃ¢u nÃ y, mÃ´ hÃ¬nh chá»‰ Ä‘Æ°a ra má»™t cÃ¢u tráº£ lá»i khÃ´ng phÃ¢n biá»‡t giá»›i tÃ­nh (waiter/waitress). Nhá»¯ng cÃ¢u tráº£ lá»i cÃ²n láº¡i lÃ  nghá» nghiá»‡p thÆ°á»ng gáº¯n liá»n vá»›i má»™t giá»›i tÃ­nh cá»¥ thá»ƒ â€” vÃ  Ä‘Ãºng váº­y, gÃ¡i máº¡i dÃ¢m Ä‘Ã£ lá»t vÃ o top 5 kháº£ nÄƒng mÃ  mÃ´ hÃ¬nh liÃªn káº¿t vá»›i â€œphá»¥ ná»¯â€ vÃ  â€œcÃ´ng viá»‡câ€. Äiá»u nÃ y xáº£y ra máº·c dÃ¹ BERT lÃ  má»™t trong sá»‘ Ã­t mÃ´ hÃ¬nh Transformer khÃ´ng Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng cÃ¡ch thu tháº­p dá»¯ liá»‡u tá»« kháº¯p nÆ¡i trÃªn internet, mÃ  thay vÃ o Ä‘Ã³ sá»­ dá»¥ng dá»¯ liá»‡u cÃ³ váº» trung láº­p (Ä‘Æ°á»£c Ä‘Ã o táº¡o trÃªn bá»™ dá»¯ liá»‡u Wikipedia tiáº¿ng Anh vÃ  BookCorpus ).

        Khi báº¡n sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ nÃ y, do Ä‘Ã³ báº¡n cáº§n pháº£i ghi nhá»› ráº±ng mÃ´ hÃ¬nh ban Ä‘áº§u báº¡n Ä‘ang sá»­ dá»¥ng cÃ³ thá»ƒ dá»… dÃ ng táº¡o ra ná»™i dung phÃ¢n biá»‡t giá»›i tÃ­nh, phÃ¢n biá»‡t chá»§ng tá»™c hoáº·c ká»³ thá»‹ ngÆ°á»i Ä‘á»“ng tÃ­nh. Viá»‡c tinh chá»‰nh mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u cá»§a báº¡n sáº½ khÃ´ng lÃ m máº¥t Ä‘i sá»± thiÃªn vá»‹ cá»‘ há»¯u nÃ y.

    - **Summary**

        CÃ¡ch tiáº¿p cáº­n cÃ¡c tÃ¡c vá»¥ NLP khÃ¡c nhau báº±ng cÃ¡ch sá»­ dá»¥ng hÃ m cáº¥p cao pipeline()tá»« ğŸ¤— Transformers. Báº¡n cÅ©ng Ä‘Ã£ tháº¥y cÃ¡ch tÃ¬m kiáº¿m vÃ  sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh trong Hub, cÅ©ngnhÆ° cÃ¡ch sá»­ dá»¥ng Inference API Ä‘á»ƒ kiá»ƒm tra cÃ¡c mÃ´ hÃ¬nh trá»±c tiáº¿p trong trÃ¬nh duyá»‡t cá»§a báº¡n.

        ChÃºng tÃ´i Ä‘Ã£ tháº£o luáº­n vá» cÃ¡ch cÃ¡c mÃ´ hÃ¬nh Transformer hoáº¡t Ä‘á»™ng á»Ÿ cáº¥p Ä‘á»™ cao vÃ  nÃ³i vá» táº§m quan trá»ng cá»§a viá»‡c há»c chuyá»ƒn giao vÃ  tinh chá»‰nh. Má»™t khÃ­a cáº¡nh quan trá»ng lÃ  báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng toÃ n bá»™ kiáº¿n â€‹â€‹trÃºc hoáº·c chá»‰ bá»™ mÃ£ hÃ³a hoáº·c bá»™ giáº£i mÃ£, tÃ¹y thuá»™c vÃ o loáº¡i nhiá»‡m vá»¥ báº¡n muá»‘n giáº£i quyáº¿t. Báº£ng sau tÃ³m táº¯t Ä‘iá»u nÃ y:

        |Model|Examples|Tasks|
        |-----|--------|-----|
        |Encoder|ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa|Sentence classification, named entity recognition, extractive question answering|
        |Decoder|CTRL, GPT, GPT-2, Transformer XL|Text generation|
        |Encoder-decoder|BART, T5, Marian, mBART|Summarization, translation, generative question answering|